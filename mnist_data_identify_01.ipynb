{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow\n",
    "import tensorflow as tf\n",
    "# import mnist data from tensorflow\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use code from \"mnist_data_identify_simple\"\n",
    "## this time, add middle layer to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter 0,Testing Accuracy 0.8962\n",
      "Iter 1,Testing Accuracy 0.9192\n",
      "Iter 2,Testing Accuracy 0.9294\n",
      "Iter 3,Testing Accuracy 0.9335\n",
      "Iter 4,Testing Accuracy 0.9382\n",
      "Iter 5,Testing Accuracy 0.9416\n",
      "Iter 6,Testing Accuracy 0.9482\n",
      "Iter 7,Testing Accuracy 0.9482\n",
      "Iter 8,Testing Accuracy 0.9503\n",
      "Iter 9,Testing Accuracy 0.9516\n",
      "Iter 10,Testing Accuracy 0.9526\n"
     ]
    }
   ],
   "source": [
    "# get mnist data set & store in current folder, rename it 'MNIST_data'\n",
    "# A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. \n",
    "# In this case, the nth digit will be represented as a vector which is 1 in the nth dimension.\n",
    "# For example, 3 would be [0,0,0,1,0,0,0,0,0,0]\n",
    "mnist = input_data.read_data_sets('MNIST_data',one_hot=True)\n",
    "\n",
    "# set batch_size, every time put 100 images for training\n",
    "batch_size = 100\n",
    "# calculate number of batches from data set\n",
    "n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "# define two placehoder\n",
    "x = tf.placeholder(tf.float32,[None,784]) #images  784 = 28 * 28 pixels\n",
    "y = tf.placeholder(tf.float32,[None,10])  #labels  10 stands for 0~9\n",
    "# ---------------------------------------------------------------------------------------------modified--------\n",
    "keep_prob = tf.placeholder(tf.float32)    # used in dropout method, definds percentage tensor works\n",
    "\n",
    "# define a nural network              \n",
    "# tf.zeros -> initial value is 0\n",
    "# W has a shape of [784, 10] because we want to multiply the 784-dimensional image vectors by it \n",
    "# to produce 10-dimensional vectors of evidence for the difference classes. \n",
    "# b:bisa, has a shape of [10] so we can add it to the output.\n",
    "# ---------------------------------------------------------------------------------------------modified--------\n",
    "W1 = tf.Variable(tf.truncated_normal([784,2000],stddev=0.1)) \n",
    "b1 = tf.Variable(tf.zeros([2000])+0.1)           # changed a initial method\n",
    "L1 = tf.nn.tanh(tf.matmul(x,W1)+b1)\n",
    "L1_drop = tf.nn.dropout(L1,keep_prob)            # L1 -> one layer output\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([2000,1000],stddev=0.1)) \n",
    "b2 = tf.Variable(tf.zeros([1000])+0.1)           \n",
    "L2 = tf.nn.tanh(tf.matmul(L1_drop,W2)+b2)\n",
    "L2_drop = tf.nn.dropout(L2,keep_prob)\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([1000,10],stddev=0.1)) \n",
    "b3 = tf.Variable(tf.zeros([10])+0.1)         \n",
    "prediction = tf.nn.tanh(tf.matmul(L2_drop,W3)+b3)\n",
    "\n",
    "# calculate cost(loss), and minimize loss\n",
    "# use cross_entropy method: tf.nn.softmax_cross_entropy_with_logits method\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))\n",
    "# use GradientDescent to train\n",
    "train_step = tf.train.GradientDescentOptimizer(0.2).minimize(loss)\n",
    "\n",
    "# initial glabal variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# calculate accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1)) # correct return true, otherwise return false\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # true->1.0   false->0\n",
    "\n",
    "# create loop to train\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for eposh in range(11):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys = mnist.train.next_batch(batch_size)\n",
    "# ---------------------------------------------------------------------------------------------modified--------\n",
    "            sess.run(train_step,{x:batch_xs, y:batch_ys, keep_prob:0.7})\n",
    "# calculate accuracy\n",
    "        test_acc = sess.run(accuracy,{x:mnist.test.images, y:mnist.test.labels, keep_prob:0.7})\n",
    "        print(\"Iter \" + str(eposh) + \",Testing Accuracy \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we added 2 layers:  784 => 2000 => 1000 => 10\n",
    "\n",
    "### And we used tf.nn.tanh method ( Computes hyperbolic tangent of x element-wise. )    \n",
    "\n",
    "there are some methods provided:\n",
    "https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/activation_functions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we add new layers, so the runing time is quite long\n",
    "result when keep_prob = 1.0 :\n",
    "Iter 0,Testing Accuracy 0.9502\n",
    "Iter 1,Testing Accuracy 0.9592\n",
    "Iter 2,Testing Accuracy 0.9669\n",
    "Iter 3,Testing Accuracy 0.9693\n",
    "Iter 4,Testing Accuracy 0.971\n",
    "Iter 5,Testing Accuracy 0.9726\n",
    "Iter 6,Testing Accuracy 0.9736\n",
    "Iter 7,Testing Accuracy 0.9744\n",
    "Iter 8,Testing Accuracy 0.9754\n",
    "Iter 9,Testing Accuracy 0.9758\n",
    "Iter 10,Testing Accuracy 0.9754\n",
    "result when keep_prob = 0.7 :\n",
    "Iter 0,Testing Accuracy 0.8962\n",
    "Iter 1,Testing Accuracy 0.9192\n",
    "Iter 2,Testing Accuracy 0.9294\n",
    "Iter 3,Testing Accuracy 0.9335\n",
    "Iter 4,Testing Accuracy 0.9382\n",
    "Iter 5,Testing Accuracy 0.9416\n",
    "Iter 6,Testing Accuracy 0.9482\n",
    "Iter 7,Testing Accuracy 0.9482\n",
    "Iter 8,Testing Accuracy 0.9503\n",
    "Iter 9,Testing Accuracy 0.9516\n",
    "Iter 10,Testing Accuracy 0.9526\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
